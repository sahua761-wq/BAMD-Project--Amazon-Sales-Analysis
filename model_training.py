# -*- coding: utf-8 -*-
"""Amazon_Sales_Report_Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bOsxqGcvgWgMZe0zkLqTVGOvg8U_x4km
"""

#Import our standard libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Preprocessing
from sklearn import preprocessing
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

#Splitting & Tuning
from sklearn.model_selection import cross_val_score, GridSearchCV

#models used
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree

#Evaluation
from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report
from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score

#suppress warnings
import warnings
warnings.filterwarnings('ignore')

#Load Clearned Dataset
fname = "Amazon_Sale_Report.csv"
df = pd.read_csv(fname, encoding='latin-1')

df.shape

df.isnull().sum()

df.head()

"""### Cleaning Data- Imputing values"""

df.info()

df["currency"].fillna("INR", inplace=True)
df["Amount"].fillna(0, inplace = True)
df["fulfilled-by"].fillna("Others", inplace = True)

df.info()

df.head()

#Cleaning the data
pd.DataFrame(df).isna().sum()

pd.DataFrame(df).nunique()

#dropping variables where most of the data is missing and date which is redundant
df = df.drop(columns = ['index','Order ID','ASIN','Courier Status','ship-postal-code','ship-country','ship-city','currency','Unnamed: 22'])

df.dropna(subset=['ship-state'], inplace=True)

df['promotion-ids'].fillna("None", inplace = True)

pd.DataFrame(df).isna().sum()

"""## Cleaning Data- Adressing errors in state names"""

df['ship-state'].value_counts()

# Visualising distribution of states
sns.catplot(x='ship-state', data = df, order = df['ship-state'].value_counts().index,kind = 'count', hue = 'ship-state',aspect = 3)
plt.xticks(rotation = 90)
plt.show()

#Addressing issue with state names
df['ship-state'] = df['ship-state'].str.upper()

df['ship-state'].unique()

df['ship-state']=df['ship-state'].replace(['NEW DELHI','DELHI'],'NEW DELHI')
df['ship-state']=df['ship-state'].replace(['ODISHA','ORISSA'],'ODISHA')
df['ship-state']=df['ship-state'].replace(['NL','NAGALAND'],'NAGALAND')
df['ship-state']=df['ship-state'].replace(['PUNJAB/MOHALI/ZIRAKPUR','PB'],'PUNJAB')
df['ship-state']=df['ship-state'].replace(['RJ','RAJSTHAN','RAJSHTHAN'],'RAJASTHAN')
df['ship-state']=df['ship-state'].replace(['PONDICHERRY','PUDUCHERRY'],'PUDUCHERRY')
df['ship-state']=df['ship-state'].replace(['AR'],'ARUNACHAL PRADESH')
df['ship-state']=df['ship-state'].replace(['APO'],'WEST BENGAL')

df['ship-state'].unique()

# Visualising distribution of states
sns.catplot(x='ship-state', data = df, order = df['ship-state'].value_counts().index,kind = 'count', hue = 'ship-state',aspect = 3)
plt.xticks(rotation = 90)
plt.show()

"""### Add Regions"""

# Mapping dictionary for Indian states & UTs to regions
region_dict = {
    'North': ['PUNJAB', 'HARYANA', 'HIMACHAL PRADESH', 'UTTARAKHAND', 'UTTAR PRADESH', 'JAMMU & KASHMIR', 'NEW DELHI', 'CHANDIGARH', 'LADAKH', 'MADHYA PRADESH'],
    'South': ['KARNATAKA', 'KERALA', 'TAMIL NADU', 'ANDHRA PRADESH', 'TELANGANA', 'PUDUCHERRY', 'LAKSHADWEEP', 'ANDAMAN & NICOBAR'],
    'East': ['BIHAR', 'ODISHA', 'WEST BENGAL', 'JHARKHAND', 'CHHATTISGARH', 'ASSAM', 'MEGHALAYA', 'MANIPUR', 'MIZORAM', 'NAGALAND', 'TRIPURA', 'ARUNACHAL PRADESH', 'SIKKIM'],
    'West': ['RAJASTHAN', 'GUJARAT', 'MAHARASHTRA', 'GOA', 'DADRA AND NAGAR'],
}
# Flatten mapping for .map()
flat_map = {state: region for region, states in region_dict.items() for state in states}
df['Region'] = df['ship-state'].map(flat_map)

df['Region'].value_counts()

df['ship-state'].value_counts()

df.info()

df['Region'].fillna('South', inplace = True)

"""### Add year & month columns"""

df['Date'] = pd.to_datetime(df['Date'])
df['Month'] = df['Date'].dt.month
df['Day_Name'] = df['Date'].dt.day_name()
df['Week'] = df['Date'].dt.isocalendar().week

df.head()

df['Month'].unique()

df['Week'].unique()

sns.catplot(x='Month', data = df, order = df['Month'].value_counts().index,kind = 'count', hue = 'Month',aspect = 3)
plt.show()

df.drop(columns = 'Date')

df.nunique()

df['promotion_flag'] = np.where(df['promotion-ids'] == 'None', 0, 1)

df.info()

print(df.columns.tolist())

region_dfs = {region: df[df['Region'] == region] for region in df['Region'].unique()}

"""## North"""

N_df = region_dfs['North'].groupby(
    ['Category', 'Size', 'B2B','promotion_flag', 'Fulfilment', 'Sales Channel ', 'fulfilled-by'],
    as_index=False
).agg({
    'Qty': 'sum',
    'Amount': 'sum'
})

N_df['Qty'].describe()

N_df['Volume_Class'] = pd.qcut(
    N_df['Qty'],
    q=2,
    labels=[0, 1],
    duplicates='drop')

sns.countplot(x='Volume_Class', data=N_df, order=[0, 1], palette='viridis')
plt.show()

N_df.info()

cat_cols = ['Category', 'Size', 'B2B', 'Fulfilment', 'Sales Channel ', 'fulfilled-by']
N_df[cat_cols] = N_df[cat_cols].astype('category')

"""### One Hot Encoding"""

#Create Independent and Dependent Variables
X1 = N_df.drop(['Qty', 'Amount', 'Volume_Class'], axis = 1)
y1 = N_df['Volume_Class']

X1 = pd.get_dummies(X1)

# Split the data into training and testing set
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=428)

print(X1_train.shape)
print(X1_test.shape)
print(y1_train.shape)
print(y1_test.shape)

"""### Logistic Regression"""

# Fit logistic regression
lr_model = LogisticRegression()
lr_model.fit(X1_train,y1_train)

# Predict for test data
predictions = lr_model.predict(X1_test)

"""### Logistic Regression Evaluation"""

# Check accuracy
print('training accuracy:', lr_model.score(X1_train,y1_train))
print('testing accuracy:', lr_model.score(X1_test,y1_test))

cm1 = confusion_matrix(y1_test,predictions)
print(cm1)

print("\n Logistic Regression Report:\n", classification_report(y1_test, predictions, target_names=['Low', 'High']))

# AUC and ROC
prediction_prob = lr_model.predict_proba(X1_test)[:,1]
false_positve_rate, true_positive_rate, thresholds = roc_curve(y1_test, prediction_prob)
roc_auc = auc(false_positve_rate, true_positive_rate)

print("AUC - ", format(roc_auc))
# Plot Precision Recall curve
plt.plot(false_positve_rate, true_positive_rate)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC Curve")
plt.show()

"""### Logistic Regression for different thresholds"""

#Try different Threshold and check precision and recall
prediction_prob = lr_model.predict_proba(X1_test)[:,1]
type(prediction_prob)
#predictions_threshold =
prediction_prob[prediction_prob > 0.8] = 1
prediction_prob[prediction_prob <= 0.8] = 0

print("\n Logistic Regression with 0.8 Threshold Report:\n", classification_report(y1_test, prediction_prob, target_names=['Low', 'High']))

prediction_prob = lr_model.predict_proba(X1_test)[:,1]
type(prediction_prob)
#predictions_threshold =
prediction_prob[prediction_prob > 0.3] = 1
prediction_prob[prediction_prob <= 0.3] = 0
print("\n Logistic Regression 0.3 threshold Report:\n", classification_report(y1_test, prediction_prob, target_names=['Low', 'High']))

"""### Decision Trees"""

#Decision Trees
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree

def print_results(results):
    print('BEST PARAMS: {}\n'.format(results.best_params_))

    means = results.cv_results_['mean_test_score']
    stds = results.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, results.cv_results_['params']):
        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))

#start with an initial guess for params
param_grid = {
    'max_depth': [10,20,30, 40],
    'min_samples_split': [20,40,60,80,100],
    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01],
    'criterion':['gini','entropy']
}

#Run Gridsearch
gridsearch = GridSearchCV(DecisionTreeClassifier(random_state = 1),
                         param_grid,
                         cv = 5,
                         n_jobs = -1)
gridsearch.fit(X1_train, y1_train)

print('Initial score:', gridsearch.best_score_)
print('Initial parameters:', gridsearch.best_params_)

#Take the best estimator
bestCtree = gridsearch.best_estimator_
predictions = bestCtree.predict(X1_test)
print(accuracy_score(y1_test, predictions))

print(classification_report(y1_test, predictions))

"""### Post-Pruning (CCP)"""

#try ccp
param_grid = {
    "ccp_alpha":[0.001, 0.005, 0.01, 0.05, 0.1]
}

gridsearch_cp = GridSearchCV(DecisionTreeClassifier(random_state = 1),
                         param_grid,
                         cv = 5,
                         n_jobs = -1)
gridsearch_cp.fit(X1_train, y1_train)

print('CCP Score:', gridsearch_cp.best_score_)
print('CCP Parameters:', gridsearch_cp.best_params_)
#Take the best estimator
bestCtree_cp = gridsearch_cp.best_estimator_
pred_cp = bestCtree_cp.predict(X1_test)
print(accuracy_score(y1_test, pred_cp))
print(classification_report(y1_test, pred_cp))

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier #RandomForestClassifier

#Create an object for random forest classifier- assumes DecisionTree
rnd_clf = RandomForestClassifier(
    n_estimators = 500,
    max_depth = 10,
    n_jobs = -1,
    random_state = 0
)

#Fit the model
rnd_clf.fit(X1_train, y1_train)

#predict using the model
rf_pred = rnd_clf.predict(X1_test)

#Check the accuracy score
print(accuracy_score(y1_test, rf_pred))

#print feature importance
features = X1_test.columns
importances = rnd_clf.feature_importances_
indices = np.argsort(importances)[::-1][:10]

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

print(classification_report(y1_test, rf_pred))

"""### Extra Trees"""

from sklearn.ensemble import ExtraTreesClassifier

#Create an object for extra tree
ext_clf = ExtraTreesClassifier(
    n_estimators = 500,
    max_depth = 10,
    n_jobs = -1,
    random_state = 0
)

ext_clf.fit(X1_train, y1_train)

ext_pred = ext_clf.predict(X1_test)

print(accuracy_score(y1_test, ext_pred))

"""## Boosting

### Adaboost
"""

from sklearn.ensemble import AdaBoostClassifier

#create an object
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth = 1),
    n_estimators = 200,
    learning_rate = 0.05,
    random_state = 0
)

ada_clf.fit(X1_train, y1_train)

ada_pred = ada_clf.predict(X1_test)
print(accuracy_score(y1_test, ada_pred))

print(classification_report(y1_test, ada_pred))

"""### XGBoost"""

from xgboost import XGBClassifier

#Create an object
xgb_clf = XGBClassifier(early_stopping_rounds = 2)

#Run the model
xgb_clf.fit(X1_train, y1_train,
            eval_set = [(X1_test, y1_test)])

#get the predictions
xgb_pred = xgb_clf.predict(X1_test)

#check for accuracy
print(accuracy_score(y1_test, xgb_pred))

print(classification_report(y1_test, xgb_pred))

"""### Choosing Best Model"""

# Accuracy Score
from sklearn.metrics import accuracy_score, precision_score, recall_score
for mdl in [lr_model, bestCtree, bestCtree_cp, rnd_clf, ext_clf, ada_clf, xgb_clf]:
    y1_pred = mdl.predict(X1_test)
    accuracy = round(accuracy_score(y1_test, y1_pred), 3)
    precision = round(precision_score(y1_test, y1_pred), 3)
    recall = round(recall_score(y1_test, y1_pred), 3)
    print('mdl: {} / # -- A: {} / P: {} / R: {}'.format(mdl.__class__.__name__,  accuracy,
                                                                         precision,
                                                                         recall))

"""### Hence Logistic Regression will be used for all other datasets"""

#Try different Threshold and check precision and recall
prediction_prob = lr_model.predict_proba(X1_test)[:,1]
type(prediction_prob)
#predictions_threshold =
for theta in np.arange(0.0, 1.05, 0.05):
    preds = (prediction_prob > theta).astype(int)  # convert to 0/1 labels
    precision = precision_score(y1_test, preds, average='binary')
    recall = recall_score(y1_test, preds, average='binary')
    print(f"Threshold: {theta:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f}")

# Get the quantity column from the original dataset for the test set rows
qty_test = N_df.loc[X1_test.index, 'Qty']  # replace 'Qty' with actual column name
amount_test = N_df.loc[X1_test.index, 'Amount']

prediction_prob = lr_model.predict_proba(X1_test)[:, 1]

results = []

for theta in np.arange(0.0, 1.05, 0.05):
    y1_pred = (prediction_prob > theta).astype(int)

    precision = precision_score(y1_test, y1_pred, pos_label=1)
    recall = recall_score(y1_test, y1_pred, pos_label=1)

    # Masks
    tp_mask = (y1_test == 1) & (y1_pred == 1)
    fp_mask = (y1_test != 1) & (y1_pred == 1)

    qty_tp = qty_test[tp_mask].sum()
    qty_fp = qty_test[fp_mask].sum()
    rev_tp = amount_test[tp_mask].sum()
    rev_fp = amount_test[fp_mask].sum()

    avg_usp_tp = (amount_test[tp_mask] / qty_test[tp_mask]).mean()
    avg_usp_fp = (amount_test[fp_mask] / qty_test[fp_mask]).mean()

    results.append({
        'Threshold': round(theta, 2),
        'Precision': round(precision, 2),
        'Recall': round(recall, 2),
        'Qty_TP': int(qty_tp),
        'Qty_FP': int(qty_fp),
        'Revenue_TP': round(rev_tp, 2),
        'Revenue_FP': round(rev_fp, 2),
        'Avg_USP_TP': round(avg_usp_tp, 2),
        'Avg_USP_FP': round(avg_usp_fp, 2)
    })

results_df = pd.DataFrame(results)
print(results_df)

"""### Threshold ≈ 0.35–0.40 seems optimal — high enough precision to avoid too many wrong positives, recall still solid, and revenue from true positives remains high while revenue from false positives is lower.

### Hence we will be going ahead with threshold of 0.4 for other regions as well

## South
"""

S_df = region_dfs['South'].groupby(
    ['Category','Size', 'B2B','promotion_flag', 'Fulfilment', 'Sales Channel ', 'fulfilled-by'],
    as_index=False
).agg({
    'Qty': 'sum',
    'Amount': 'sum'
})

S_df['Volume_Class'] = pd.qcut(
    S_df['Qty'],
    q=2,
    labels=[0, 1],
    duplicates='drop')

sns.countplot(x='Volume_Class', data=S_df, order=[0, 1], palette='viridis')
plt.show()

cat_cols = ['Category', 'Size', 'B2B', 'Fulfilment', 'Sales Channel ', 'fulfilled-by']
S_df[cat_cols] = S_df[cat_cols].astype('category')

#Create Independent and Dependent Variables
X2 = S_df.drop(['Qty', 'Amount', 'Volume_Class'], axis = 1)
y2 = S_df['Volume_Class']

X2 = pd.get_dummies(X2)

# Split the data into training and testing set
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=428)

print(X2_train.shape)
print(X2_test.shape)
print(y2_train.shape)
print(y2_test.shape)

# Fit logistic regression
lr_model2 = LogisticRegression()
lr_model2.fit(X2_train,y2_train)

prediction_probS = lr_model2.predict_proba(X2_test)[:,1]
type(prediction_probS)
#predictions_threshold =
prediction_probS[prediction_probS > 0.4] = 1
prediction_probS[prediction_probS <= 0.4] = 0

print("\n Logistic Regression 0.4 threshold Report:\n", classification_report(y2_test, prediction_probS, target_names=['Low', 'High']))

"""## West"""

W_df = region_dfs['West'].groupby(
    ['Category', 'Size', 'B2B','promotion_flag', 'Fulfilment', 'Sales Channel ', 'fulfilled-by'],
    as_index=False
).agg({
    'Qty': 'sum',
    'Amount': 'sum'
})

W_df['Volume_Class'] = pd.qcut(
    W_df['Qty'],
    q=2,
    labels=[0, 1],
    duplicates='drop')

sns.countplot(x='Volume_Class', data=W_df, order=[0, 1], palette='viridis')
plt.show()

W_df[cat_cols] = W_df[cat_cols].astype('category')

#Create Independent and Dependent Variables
X3 = W_df.drop(['Qty', 'Amount', 'Volume_Class', ], axis = 1)
y3 = W_df['Volume_Class']

X3 = pd.get_dummies(X3)

# Split the data into training and testing set
X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=428)

print(X3_train.shape)
print(X3_test.shape)
print(y3_train.shape)
print(y3_test.shape)

# Fit logistic regression
lr_model3 = LogisticRegression()
lr_model3.fit(X3_train,y3_train)

prediction_probW = lr_model3.predict_proba(X3_test)[:,1]
type(prediction_probW)
#predictions_threshold =
prediction_probW[prediction_probW > 0.4] = 1
prediction_probW[prediction_probW <= 0.4] = 0

print("\n Logistic Regression 0.4 threshold Report:\n", classification_report(y3_test, prediction_probW, target_names=['Low', 'High']))

"""## East"""

E_df = region_dfs['East'].groupby(
    ['Category', 'Size', 'B2B','promotion_flag', 'Fulfilment', 'Sales Channel ', 'fulfilled-by'],
    as_index=False
).agg({
    'Qty': 'sum',
    'Amount': 'sum'
})

E_df['Volume_Class'] = pd.qcut(
    E_df['Qty'],
    q=2,
    labels=[0, 1],
    duplicates='drop')

sns.countplot(x='Volume_Class', data=E_df, order=[0, 1], palette='viridis')
plt.show()

E_df[cat_cols] = E_df[cat_cols].astype('category')

#Create Independent and Dependent Variables
X4 = E_df.drop(['Qty', 'Amount', 'Volume_Class'], axis = 1)
y4 = E_df['Volume_Class']

X4 = pd.get_dummies(X4)

# Split the data into training and testing set
X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=428)

print(X4_train.shape)
print(X4_test.shape)
print(y4_train.shape)
print(y4_test.shape)

# Fit logistic regression
lr_model4 = LogisticRegression()
lr_model4.fit(X4_train,y4_train)

prediction_probE = lr_model4.predict_proba(X4_test)[:,1]
type(prediction_probE)
#predictions_threshold =
prediction_probE[prediction_probE > 0.4] = 1
prediction_probE[prediction_probE <= 0.4] = 0

print("\n Logistic Regression 0.4 threshold Report:\n", classification_report(y4_test, prediction_probE, target_names=['Low', 'High']))

# Export variables so Streamlit can use them
__all__ = ["N_df", "S_df", "E_df", "W_df", "lr_model", "lr_model2", "lr_model3", "lr_model4", "df"]